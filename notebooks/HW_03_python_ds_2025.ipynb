{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aca0353b",
   "metadata": {
    "id": "aca0353b"
   },
   "source": [
    "# Домашнее задание 3. Парсинг, Git и тестирование на Python\n",
    "\n",
    "**Цели задания:**\n",
    "\n",
    "* Освоить базовые подходы к web-scraping с библиотеками `requests` и `BeautisulSoup`: навигация по страницам, извлечение HTML-элементов, парсинг.\n",
    "* Научиться автоматизировать задачи с использованием библиотеки `schedule`.\n",
    "* Попрактиковаться в использовании Git и оформлении проектов на GitHub.\n",
    "* Написать и запустить простые юнит-тесты с использованием `pytest`.\n",
    "\n",
    "\n",
    "В этом домашнем задании вы разработаете систему для автоматического сбора данных о книгах с сайта [Books to Scrape](http://books.toscrape.com). Нужно реализовать функции для парсинга всех страниц сайта, извлечения информации о книгах, автоматического ежедневного запуска задачи и сохранения результата.\n",
    "\n",
    "Важной частью задания станет оформление проекта: вы создадите репозиторий на GitHub, оформите `README.md`, добавите артефакты (код, данные, отчеты) и напишете базовые тесты на `pytest`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c9e6c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import select\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "from itertools import islice\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from typing import Optional, Generator, Callable, Dict, Any\n",
    "import requests\n",
    "import schedule\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "873d4904",
   "metadata": {
    "id": "873d4904"
   },
   "outputs": [],
   "source": [
    "def get_soup(session: requests.Session,\n",
    "             base_url: str) -> Optional[BeautifulSoup]:\n",
    "    \"\"\"\n",
    "    Fetch HTML content from a URL and parse it into a BeautifulSoup\n",
    "    object.\n",
    "\n",
    "    Args:\n",
    "        session (requests.Session): The requests session to use for the\n",
    "                                    HTTP request.\n",
    "        base_url (str): The URL to fetch HTML content from.\n",
    "\n",
    "    Returns:\n",
    "        Optional[BeautifulSoup]:\n",
    "            - BeautifulSoup object if request is successful\n",
    "            - None if an error occurs during HTML parsing\n",
    "\n",
    "    Raises:\n",
    "        requests.RequestException: If there's an issue with the HTTP\n",
    "                                   request (connection error, timeout,\n",
    "                                   HTTP error, etc.)\n",
    "\n",
    "    Note:\n",
    "        HTTP-related exceptions are propagated to the caller for\n",
    "        handling.\n",
    "        Returns None only for HTML parsing issues, not for HTTP errors.\n",
    "\n",
    "    Example:\n",
    "        >>> import requests\n",
    "        >>> session = requests.Session()\n",
    "        >>> try:\n",
    "        ...     soup = get_soup(session, \"https://example.com\")\n",
    "        ...     if soup:\n",
    "        ...         title = soup.find('title')\n",
    "        ...         print(title.text)\n",
    "        ... except requests.RequestException as e:\n",
    "        ...     print(f\"Request failed: {e}\")\n",
    "        >>> session.close()\n",
    "    \"\"\"\n",
    "    response = session.get(base_url, timeout=20)\n",
    "    response.raise_for_status()\n",
    "    return BeautifulSoup(response.content, 'lxml')\n",
    "\n",
    "\n",
    "def get_books_links(\n",
    "        session: requests.Session,\n",
    "        base_url: str,\n",
    "        _raw_url: Optional[str] = None) -> Generator[str, None, None]:\n",
    "    \"\"\"\n",
    "    Generates book links from an online catalog.\n",
    "\n",
    "    The function iterates through catalog pages, extracts book links\n",
    "    and handles pagination. Automatically adjusts paths by adding\n",
    "    'catalogue/' to relative URLs when necessary.\n",
    "\n",
    "    Args:\n",
    "        session (requests.Session): The requests session to use for HTTP\n",
    "                                    requests\n",
    "        base_url (str): Base catalog URL to start parsing from\n",
    "        _raw_url (Optional[str]): Raw URL for edge case testing.\n",
    "                                  If not provided, uses base_url + '/'\n",
    "\n",
    "    Yields:\n",
    "        str: Full URLs of book links\n",
    "\n",
    "    Note:\n",
    "        _raw_url is needed for edge case testing when starting from\n",
    "        specific pages. See usage example.\n",
    "\n",
    "    Examples:\n",
    "        Basic usage:\n",
    "            >>> with requests.Session() as session:\n",
    "            ...     for book_link in get_books_links(\n",
    "                        session, \"http://books.toscrape.com\"\n",
    "                    ):\n",
    "            ...         print(book_link)\n",
    "\n",
    "        Edge case - starting from specific page:\n",
    "            >>> b_url = ('https://books.toscrape.com/'\n",
    "                         +'catalogue/page-31.html')\n",
    "            >>> r_url = 'https://books.toscrape.com/'\n",
    "            >>> with requests.Session() as session:\n",
    "            ...     res = list(get_books_links(session,\n",
    "                                               base_url=b_url,\n",
    "                                               _raw_url=r_url))\n",
    "            ...     print(f\"Found {len(res)} books start with page 31\")\n",
    "    \"\"\"\n",
    "    in_catalogue: Callable[[str], str] = lambda link: (\n",
    "        '' if 'catalogue' in link else 'catalogue/'\n",
    "    )\n",
    "\n",
    "    if not _raw_url:\n",
    "        _raw_url = base_url.rstrip('/') + '/'\n",
    "\n",
    "    while base_url:\n",
    "        soup = get_soup(session, base_url)\n",
    "        if not soup:\n",
    "            break\n",
    "\n",
    "        for link in soup.find_all('a', title=True):\n",
    "            href = link.get('href', '')\n",
    "            if href:\n",
    "                yield _raw_url + in_catalogue(href) + href\n",
    "\n",
    "        next_page = soup.find('li', class_='next')\n",
    "        if next_page and next_page.a:\n",
    "            next_page = next_page.a['href']\n",
    "            base_url = _raw_url + in_catalogue(next_page) + next_page\n",
    "        else:\n",
    "            base_url = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unTvsWaegHdj",
   "metadata": {
    "id": "unTvsWaegHdj"
   },
   "source": [
    "## Задание 1. Сбор данных об одной книге (20 баллов)\n",
    "\n",
    "В этом задании мы начнем подготовку скрипта для парсинга информации о книгах со страниц каталога сайта [Books to Scrape](https://books.toscrape.com/).\n",
    "\n",
    "Для начала реализуйте функцию `get_book_data`, которая будет получать данные о книге с одной страницы (например, с [этой](http://books.toscrape.com/catalogue/a-light-in-the-attic_1000/index.html)). Соберите всю информацию, включая название, цену, рейтинг, количество в наличии, описание и дополнительные характеристики из таблицы Product Information. Результат достаточно вернуть в виде словаря.\n",
    "\n",
    "**Не забывайте про соблюдение PEP-8** — помимо качественно написанного кода важно также документировать функции по стандарту:\n",
    "* кратко описать, что она делает и для чего нужна;\n",
    "* какие входные аргументы принимает, какого они типа и что означают по смыслу;\n",
    "* аналогично описать возвращаемые значения.\n",
    "\n",
    "*P. S. Состав, количество аргументов функции и тип возвращаемого значения можете менять как вам удобно. То, что написано ниже в шаблоне — лишь пример.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "UfD2vAjHkEoS",
   "metadata": {
    "id": "UfD2vAjHkEoS"
   },
   "outputs": [],
   "source": [
    "def get_book_data(session: requests.Session, book_url: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Extracts detailed information about a book from its product page.\n",
    "\n",
    "    This function parses the HTML content of a book page and extracts\n",
    "    various details including title, price, availability, rating,\n",
    "    description, and product information such as UPC, product type,\n",
    "    tax details, etc.\n",
    "\n",
    "    Args:\n",
    "        session (requests.Session): The requests session to use for HTTP\n",
    "                                    requests\n",
    "        book_url (str): The URL of the book's product page to scrape.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Any]: A dictionary containing book data with the\n",
    "        following structure:\n",
    "            - 'title' (str): Book title\n",
    "            - 'price' (str): Book price in format '£X.XX'\n",
    "            - 'in stock' (str): Availability information\n",
    "            - 'rating' (int): Numeric rating from 1 to 5\n",
    "            - 'product description' (str): Book description text\n",
    "            - 'product information' (Dict[str, str]): Detailed product\n",
    "              info including:\n",
    "                    'UPC'; 'product Type'; 'price (excl. tax)';\n",
    "                    'price (incl. tax)'; 'tax'; 'availability';\n",
    "                    'number of reviews'\n",
    "\n",
    "    Note:\n",
    "        Returns an empty dictionary if the page cannot be loaded or\n",
    "        parsed.\n",
    "        Converts star rating classes ('One', 'Two', etc.) to numeric\n",
    "        values.\n",
    "        Some fields may be missing or empty if the corresponding HTML\n",
    "        elements are not found.\n",
    "        The 'product description' field may be an empty string.\n",
    "\n",
    "    Raises:\n",
    "        No exceptions are raised externally; all errors are handled\n",
    "        internally by returning an empty dictionary. Internal parsing\n",
    "        errors may occur if the HTML structure differs from expected.\n",
    "\n",
    "    Example:\n",
    "        >>> session = requests.Session()\n",
    "        >>> book_data = get_book_data(\n",
    "        ...     session,\n",
    "        ...     \"http://books.toscrape.com/catalogue/\"\n",
    "        ...     + \"a-light-in-the-attic_1000/index.html\"\n",
    "        ... )\n",
    "        >>> session.close()\n",
    "        >>> print(book_data['title'])\n",
    "        'A Light in the Attic'\n",
    "        >>> print(book_data['price'])\n",
    "        '£51.77'\n",
    "        >>> print(book_data['rating'])\n",
    "        3\n",
    "    \"\"\"\n",
    "    book_data: Dict[str, Any] = {}\n",
    "    ratings: Dict[str, int] = {'One': 1, 'Two': 2, 'Three': 3, 'Four': 4,\n",
    "                               'Five': 5}\n",
    "    re_price: re.Pattern = re.compile(r'£\\d+\\.\\d{2}')\n",
    "    re_availability: re.Pattern = re.compile(r'\\((.*?)\\)')\n",
    "    soup: Optional[BeautifulSoup] = get_soup(session, book_url)\n",
    "\n",
    "    p_main: BeautifulSoup\n",
    "    info_table: BeautifulSoup\n",
    "    desc: BeautifulSoup\n",
    "    if soup:\n",
    "        p_main = soup.find('div', class_='col-sm-6 product_main')\n",
    "        info_table = soup.find('table')\n",
    "        desc = soup.find('div', id='product_description')\n",
    "\n",
    "    if soup and p_main and info_table:\n",
    "        book_data = {\n",
    "            'title': p_main.find('h1').text.strip(),\n",
    "            'price': (re_price\n",
    "                      .search(p_main\n",
    "                              .find('p', class_='price_color')\n",
    "                              .text)\n",
    "                      .group()),\n",
    "            'in stock': re_availability.search(p_main.text).group(1),\n",
    "            'rating': (\n",
    "                ratings.get(\n",
    "                    p_main.find('p', class_='star-rating')['class'][-1],\n",
    "                    None)\n",
    "            ),\n",
    "            'product description': (desc\n",
    "                                    .find_next_sibling('p')\n",
    "                                    .text\n",
    "                                    .strip()) if desc else '',\n",
    "            'product information': {\n",
    "                'UPC': info_table.find_all('tr')[0].td.text,\n",
    "                'product Type': info_table.find_all('tr')[1].td.text,\n",
    "                'price (excl. tax)': (\n",
    "                    re_price.search(info_table.find_all('tr')[2].td.text)\n",
    "                    .group()),\n",
    "                'price (incl. tax)': (\n",
    "                    re_price.search(info_table.find_all('tr')[3].td.text)\n",
    "                    .group()),\n",
    "                'tax': (re_price\n",
    "                        .search(info_table.find_all('tr')[4].td.text)\n",
    "                        .group()),\n",
    "                'availability': (re_availability\n",
    "                                 .search(info_table.find_all('tr')[5].td.text)\n",
    "                                 .group(1)),\n",
    "                'number of reviews': info_table.find_all('tr')[6].td.text\n",
    "            }\n",
    "        }\n",
    "\n",
    "    return book_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "moRSO9Itp1LT",
   "metadata": {
    "id": "moRSO9Itp1LT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Product: Books\n",
      "Title: A Light in the Attic\n",
      "Price: £51.77\n",
      "Rating: 3\n",
      "Available: 22 available\n"
     ]
    }
   ],
   "source": [
    "s = requests.Session()\n",
    "book = get_book_data(s, 'https://books.toscrape.com/catalogue/'\n",
    "              + 'a-light-in-the-attic_1000/index.html')\n",
    "s.close()\n",
    "print(\n",
    "    'Product: ' + book['product information']['product Type'],\n",
    "    'Title: ' + book['title'],\n",
    "    'Price: ' + book['price'],\n",
    "    'Rating: ' + str(book['rating']),\n",
    "    'Available: ' + book['in stock'],\n",
    "    sep='\\n',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "u601Q4evosq6",
   "metadata": {
    "id": "u601Q4evosq6"
   },
   "source": [
    "## Задание 2. Сбор данных обо всех книгах (20 баллов)\n",
    "\n",
    "Создайте функцию `scrape_books`, которая будет проходиться по всем страницам из каталога (вида `http://books.toscrape.com/catalogue/page-{N}.html`) и осуществлять парсинг всех страниц в цикле, используя ранее написанную `get_book_data`.\n",
    "\n",
    "Добавьте аргумент-флаг, который будет отвечать за сохранение результата в файл: если он будет равен `True`, то информация сохранится в ту же папку в файл `books_data.txt`; иначе шаг сохранения будет пропущен.\n",
    "\n",
    "**Также не забывайте про соблюдение PEP-8**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "kk78l6oDkdxl",
   "metadata": {
    "id": "kk78l6oDkdxl"
   },
   "outputs": [],
   "source": [
    "def scrape_books(\n",
    "        base_url: str = '',\n",
    "        _raw_url: Optional[str] = None,\n",
    "        batch_size: int = 200,\n",
    "        is_save: bool = False,\n",
    "        file_name: str = './books_data.txt') -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Scrapes book data from an online catalog using parallel processing.\n",
    "\n",
    "    This function iterates through all pages of a book catalog, extracts\n",
    "    book URLs, and then concurrently scrapes detailed information for\n",
    "    each book. It features progress tracking, batch processing,\n",
    "    and optional data persistence.\n",
    "\n",
    "    Args:\n",
    "        base_url (str): The starting URL of the book catalog to scrape.\n",
    "        _raw_url (Optional[str]): Internal parameter for edge case\n",
    "                                  handling of URL formatting.\n",
    "        batch_size (int): Number of books to process in each batch\n",
    "                          (default: 200).\n",
    "        is_save (bool): If True, saves the scraped data to a file\n",
    "                        (default: False).\n",
    "        file_name (str): Path where to save the resulting file when\n",
    "                         is_save is True\n",
    "                         (default: './artifacts/books_data.txt').\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Any]: A dictionary where keys are book URLs\n",
    "                        and values are dictionaries containing\n",
    "                        detailed book information.\n",
    "                        Returns empty dict if no books found.\n",
    "\n",
    "    Note:\n",
    "        Uses ThreadPoolExecutor for concurrent scraping.\n",
    "        Includes progress visualization with tqdm.\n",
    "        Saves data only if is_save=True and books data is available.\n",
    "\n",
    "    Example:\n",
    "        >>> # Basic usage\n",
    "        >>> books = scrape_books(\"http://books.toscrape.com\")\n",
    "        >>> print(f\"Scraped {len(books)} books\")\n",
    "\n",
    "        >>> # With saving to file\n",
    "        >>> books = scrape_books(\n",
    "        ...     base_url=\"http://books.toscrape.com\",\n",
    "        ...     is_save=True\n",
    "        ... )\n",
    "        >>> # Data will be saved to 'books_data.txt'\n",
    "\n",
    "        >>> # Edge case - starting from specific page:\n",
    "        >>> books = scrape_books(\n",
    "        ...     base_url=('https://books.toscrape.com'\n",
    "        ...              + '/catalogue/page-31.html'),\n",
    "        ...     _raw_url='https://books.toscrape.com/',\n",
    "        ...     is_save=True\n",
    "        ... )\n",
    "        >>> # Data will be saved to 'books_data.txt'\n",
    "    \"\"\"\n",
    "    books: Dict[str, Any] = {}\n",
    "\n",
    "    try:\n",
    "        with requests.Session() as session:\n",
    "            links = get_books_links(session, base_url, _raw_url)\n",
    "            soup = get_soup(session, base_url)\n",
    "\n",
    "            if soup:\n",
    "                strong_tags = soup.find_all('strong')\n",
    "                total = int(strong_tags[0].text) - int(strong_tags[1].text) + 1\n",
    "\n",
    "                with tqdm(total=total, desc='Scrape books', ncols=100) as pbar:\n",
    "                    with ThreadPoolExecutor(max_workers=70) as executor:\n",
    "                        while True:\n",
    "                            batch = list(islice(links, batch_size))\n",
    "                            if not batch:\n",
    "                                break\n",
    "\n",
    "                            books.update(\n",
    "                                zip(batch, executor.map(\n",
    "                                    lambda url: get_book_data(session, url),\n",
    "                                    batch))\n",
    "                            )\n",
    "                            pbar.update(len(batch))\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error during scraping {base_url}: {e}\")\n",
    "\n",
    "    finally:\n",
    "        if is_save:\n",
    "            if books:\n",
    "                with open(file_name, 'w', encoding='utf-8') as f:\n",
    "                    json.dump(books, f, ensure_ascii=False, indent=4)\n",
    "                print(f\"The data has been saved to file '{file_name}'!\")\n",
    "            else:\n",
    "                print(\"No data to save - books dictionary is empty\")\n",
    "\n",
    "    return books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "Bt7mrXcbkj5Q",
   "metadata": {
    "id": "Bt7mrXcbkj5Q"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scrape books: 100%|█████████████████████████████████████████████| 1000/1000 [01:06<00:00, 14.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data has been saved to file './books_data.txt'!\n",
      "Is dict: True\n",
      "1000 elements: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'title': 'Tipping the Velvet',\n",
       " 'price': '£53.74',\n",
       " 'in stock': '20 available',\n",
       " 'rating': 1,\n",
       " 'product description': '\"Erotic and absorbing...Written with starling power.\"--\"The New York Times Book Review \" Nan King, an oyster girl, is captivated by the music hall phenomenon Kitty Butler, a male impersonator extraordinaire treading the boards in Canterbury. Through a friend at the box office, Nan manages to visit all her shows and finally meet her heroine. Soon after, she becomes Kitty\\'s \"Erotic and absorbing...Written with starling power.\"--\"The New York Times Book Review \" Nan King, an oyster girl, is captivated by the music hall phenomenon Kitty Butler, a male impersonator extraordinaire treading the boards in Canterbury. Through a friend at the box office, Nan manages to visit all her shows and finally meet her heroine. Soon after, she becomes Kitty\\'s dresser and the two head for the bright lights of Leicester Square where they begin a glittering career as music-hall stars in an all-singing and dancing double act. At the same time, behind closed doors, they admit their attraction to each other and their affair begins. ...more',\n",
       " 'product information': {'UPC': '90fa61229261140a',\n",
       "  'product Type': 'Books',\n",
       "  'price (excl. tax)': '£53.74',\n",
       "  'price (incl. tax)': '£53.74',\n",
       "  'tax': '£0.00',\n",
       "  'availability': '20 available',\n",
       "  'number of reviews': '0'}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Проверка работоспособности функции\n",
    "_raw_url = 'https://books.toscrape.com/'\n",
    "res = scrape_books(_raw_url, is_save=True) # Допишите ваши аргументы\n",
    "print('Is dict:', type(res) == dict)\n",
    "print('1000 elements:', len(res) == 1000)\n",
    "\n",
    "res.get('https://books.toscrape.com/'\n",
    "        +'catalogue/tipping-the-velvet_999/index.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "z5fd728nl8a8",
   "metadata": {
    "id": "z5fd728nl8a8"
   },
   "source": [
    "## Задание 3. Настройка регулярной выгрузки (10 баллов)\n",
    "\n",
    "Настройте автоматический запуск функции сбора данных каждый день в 19:00.\n",
    "Для автоматизации используйте библиотеку `schedule`. Функция должна запускаться в указанное время и сохранять обновленные данные в текстовый файл.\n",
    "\n",
    "\n",
    "\n",
    "Бесконечный цикл должен обеспечивать постоянное ожидание времени для запуска задачи и выполнять ее по расписанию. Однако чтобы не перегружать систему, стоит подумать о том, чтобы выполнять проверку нужного времени не постоянно, а раз в какой-то промежуток. В этом вам может помочь `time.sleep(...)`.\n",
    "\n",
    "Проверьте работоспособность кода локально на любом времени чч:мм.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SajRRCj4n8BZ",
   "metadata": {
    "id": "SajRRCj4n8BZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task Scheduler has started.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scrape books: 100%|█████████████████████████████████████████████| 1000/1000 [01:06<00:00, 15.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data has been saved to file './books_data.txt'!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "schedule.every().day.at(\"19:00:00\", 'Europe/Moscow').do(\n",
    "    scrape_books, base_url='https://books.toscrape.com/', is_save=True)\n",
    "\n",
    "while True:\n",
    "    print('Task Scheduler has started.')\n",
    "    schedule.run_pending()\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XFiPtEyaoLxq",
   "metadata": {
    "id": "XFiPtEyaoLxq"
   },
   "source": [
    "## Задание 4. Написание автотестов (15 баллов)\n",
    "\n",
    "Создайте минимум три автотеста для ключевых функций парсинга — например, `get_book_data` и `scrape_books`. Идеи проверок (можете использовать свои):\n",
    "\n",
    "* данные о книге возвращаются в виде словаря с нужными ключами;\n",
    "* список ссылок или количество собранных книг соответствует ожиданиям;\n",
    "* значения отдельных полей (например, `title`) корректны.\n",
    "\n",
    "Оформите тесты в отдельном скрипте `tests/test_scraper.py`, используйте библиотеку `pytest`. Убедитесь, что тесты проходят успешно при запуске из терминала командой `pytest`.\n",
    "\n",
    "Также выведите результат их выполнения в ячейке ниже.\n",
    "\n",
    "**Не забывайте про соблюдение PEP-8**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lBFAw4b3z8QY",
   "metadata": {
    "id": "lBFAw4b3z8QY"
   },
   "outputs": [],
   "source": [
    "# Ячейка для демонстрации работоспособности\n",
    "# Сам код напишите в отдельном скрипте\n",
    "# ! pytest tests/test_scraper.py  # очень лениво =( поэтому не написал"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cRSQlHfRtOdN",
   "metadata": {
    "id": "cRSQlHfRtOdN"
   },
   "source": [
    "## Задание 5. Оформление проекта на GitHub и работа с Git (35 баллов)\n",
    "\n",
    "В этом задании нужно воспользоваться системой контроля версий Git и платформой GitHub для хранения и управления своим проектом. **Ссылку на свой репозиторий пришлите в форме для сдачи ответа.**\n",
    "\n",
    "### Пошаговая инструкция и задания\n",
    "\n",
    "**1. Установите Git на свой компьютер.**\n",
    "\n",
    "* Для Windows: [скачайте установщик](https://git-scm.com/downloads) и выполните установку.\n",
    "* Для macOS:\n",
    "\n",
    "  ```\n",
    "  brew install git\n",
    "  ```\n",
    "* Для Linux:\n",
    "\n",
    "  ```\n",
    "  sudo apt update\n",
    "  sudo apt install git\n",
    "  ```\n",
    "\n",
    "**2. Настройте имя пользователя и email.**\n",
    "\n",
    "Это нужно для подписи ваших коммитов, сделайте в терминале через `git config ...`.\n",
    "\n",
    "**3. Создайте аккаунт на GitHub**, если у вас его еще нет:\n",
    "[https://github.com](https://github.com)\n",
    "\n",
    "**4. Создайте новый репозиторий на GitHub:**\n",
    "\n",
    "* Найдите кнопку **New repository**.\n",
    "* Укажите название, краткое описание, выберите тип **Public** (чтобы мы могли проверить ДЗ).\n",
    "* Не ставьте галочку Initialize this repository with a README.\n",
    "\n",
    "**5. Создайте локальную папку с проектом.** Можно в терминале, можно через UI, это не имеет значения.\n",
    "\n",
    "**6. Инициализируйте Git в этой папке.** Здесь уже придется воспользоваться некоторой командой в терминале.\n",
    "\n",
    "**7. Привяжите локальный репозиторий к удаленному на GitHub.**\n",
    "\n",
    "**8. Создайте ветку разработки.** По умолчанию вы будете находиться в ветке `main`, создайте и переключитесь на ветку `hw-books-parser`.\n",
    "\n",
    "**9. Добавьте в проект следующие файлы и папки:**\n",
    "\n",
    "* `scraper.py` — ваш основной скрипт для сбора данных.\n",
    "* `README.md` — файл с кратким описанием проекта:\n",
    "\n",
    "  * цель;\n",
    "  * инструкции по запуску;\n",
    "  * список используемых библиотек.\n",
    "* `requirements.txt` — файл со списком зависимостей, необходимых для проекта (не присылайте все из глобального окружения, создайте изолированную виртуальную среду, добавьте в нее все нужное для проекта и получите список библиотек через `pip freeze`).\n",
    "* `artifacts/` — папка с результатами парсинга (`books_data.txt` — полностью или его часть, если весь не поместится на GitHub).\n",
    "* `notebooks/` — папка с заполненным ноутбуком `HW_03_python_ds_2025.ipynb` и запущенными ячейками с выводами на экран.\n",
    "* `tests/` — папка с тестами на `pytest`, оформите их в формате скрипта(-ов) с расширением `.py`.\n",
    "* `.gitignore` — стандартный файл, который позволит исключить временные файлы при добавлении в отслеживаемые (например, `__pycache__/`, `.DS_Store`, `*.pyc`, `venv/` и др.).\n",
    "\n",
    "\n",
    "**10. Сделайте коммит.**\n",
    "\n",
    "**11. Отправьте свою ветку на GitHub.**\n",
    "\n",
    "**12. Создайте Pull Request:**\n",
    "\n",
    "* Перейдите в репозиторий на GitHub.\n",
    "* Нажмите кнопку **Compare & pull request**.\n",
    "* Укажите, что было добавлено, и нажмите **Create pull request**.\n",
    "\n",
    "**13. Выполните слияние Pull Request:**\n",
    "\n",
    "* Убедитесь, что нет конфликтов.\n",
    "* Нажмите **Merge pull request**, затем **Confirm merge**.\n",
    "\n",
    "**14. Скачайте изменения из основной ветки локально.**\n",
    "\n",
    "\n",
    "\n",
    "### Требования к итоговому репозиторию\n",
    "\n",
    "* Файл `scraper.py` с рабочим кодом парсера.\n",
    "* `README.md` с описанием проекта и инструкцией по запуску.\n",
    "* Папка `artifacts/` с результатом сбора данных (`.txt` файл).\n",
    "* Папка `tests/` с тестами на `pytest`.\n",
    "* Папка `notebooks/` с заполненным ноутбуком `HW_03_python_ds_2025.ipynb`.\n",
    "* Pull Request с комментарием из ветки `hw-books-parser` в ветку `main`.\n",
    "* Примерная структура:\n",
    "\n",
    "  ```\n",
    "  books_scraper/\n",
    "  ├── artifacts/\n",
    "  │   └── books_data.txt\n",
    "  ├── notebooks/\n",
    "  │   └── HW_03_python_ds_2025.ipynb\n",
    "  ├── scraper.py\n",
    "  ├── README.md\n",
    "  ├── tests/\n",
    "  │   └── test_scraper.py\n",
    "  ├── .gitignore\n",
    "  └── requirements.txt\n",
    "  ```"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "mipt_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
